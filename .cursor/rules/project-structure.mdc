---
alwaysApply: true
---

# LegalBench Project Overview and Navigation

- Core modules:
  - [tasks.py](mdc:tasks.py): Declares task name groups (issue, rule, conclusion, interpretation, rhetoric).
  - [evaluation.py](mdc:evaluation.py): Implements evaluation metrics per task type (balanced accuracy default; task-specific handlers).
  - [utils.py](mdc:utils.py): Prompt utilities, including `generate_prompts()` for filling templates.
- Tasks live under `tasks/{task_name}/` with:
  - `README.md` (task description)
  - `base_prompt.txt` (baseline prompt template)
  - `train.tsv`, `test.tsv` (tab-separated data)
  - Optional prompt variants (e.g., `application_prompt.txt`, `rule_description_prompt.txt`)
- Prompts use `{{column_name}}` placeholders that map to TSV columns.
- Data format: TSV (tab-separated). Some filenames elsewhere may end with `.jsonl`, but LegalBench task data are TSV.
- Helpful references:
  - Project readme: [README.md](mdc:README.md)
  - Contributor guidance and evaluation specifics: [CLAUDE.md](mdc:CLAUDE.md)
  - Agent-evaluation implementation guide and examples: [.docs/legalbench_implementation_alpha.md](mdc:.docs/legalbench_implementation_alpha.md)

Usage notes:

- When adding or modifying tasks, preserve TSV structure and column names expected by prompts.
- When adjusting evaluation logic, keep metric mappings consistent with [evaluation.py](mdc:evaluation.py) and the paper.
